{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tokenisation, Stemming and PoS",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOW61zUt2Bpf"
      },
      "source": [
        "## *Tokenization*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VP-7gC52a-d",
        "outputId": "5c2272db-4fde-4003-f9ce-64fe0a8dcef7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XDQjCKSQ19QO",
        "outputId": "eb483001-40f3-4239-cdc9-1f6bea1d0b27"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. The sky is pinkish-blue. You shouldn't eat cardboard.\"\n",
        "print(sent_tokenize(EXAMPLE_TEXT))\n",
        "print(word_tokenize(EXAMPLE_TEXT))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n",
            "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDwbjxja2gFo"
      },
      "source": [
        "## *Stemming*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuUm8amX2o7L"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1cGzt1p2OJL",
        "outputId": "6b2211d8-d92b-423b-c0ae-94a9fa848f41"
      },
      "source": [
        "ps = PorterStemmer()\n",
        "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
        "for w in example_words:\n",
        "  print(ps.stem(w))\n",
        "\n",
        "new_text = \"It is important to by very pythonly while you are pythoning with python. All pythoners have pythoned poorly at least once.\"\n",
        "words = word_tokenize(new_text)\n",
        "\n",
        "for w in words:\n",
        "  print(ps.stem(w))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python\n",
            "python\n",
            "python\n",
            "python\n",
            "pythonli\n",
            "It\n",
            "is\n",
            "import\n",
            "to\n",
            "by\n",
            "veri\n",
            "pythonli\n",
            "while\n",
            "you\n",
            "are\n",
            "python\n",
            "with\n",
            "python\n",
            ".\n",
            "all\n",
            "python\n",
            "have\n",
            "python\n",
            "poorli\n",
            "at\n",
            "least\n",
            "onc\n",
            ".\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZeHy8s1X250h"
      },
      "source": [
        "## *PoS*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRkaibqu3f0Z",
        "outputId": "b12ca356-22a2-4730-cc55-bb24dd55662c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('state_union')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s700aXyo3C2e"
      },
      "source": [
        "from nltk.corpus import state_union\n",
        "from nltk.tokenize import PunktSentenceTokenizer"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RXwSmGw2wHC",
        "outputId": "fe5a2e24-c14c-47b2-9e7c-53ac86bd965e"
      },
      "source": [
        "train_text = state_union.raw(\"2005-GWBush.txt\") #2005-GWBush.txt is available online for download\n",
        "sample_text = state_union.raw(\"2006-GWBush.txt\") #2006-GWBush.txt is available online for download\n",
        "\n",
        "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
        "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
        "\n",
        "def process_content():\n",
        "  try:\n",
        "    for i in tokenized[:5]:\n",
        "      words = nltk.word_tokenize(i)\n",
        "      tagged = nltk.pos_tag(words)\n",
        "      print(tagged)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(str(e))\n",
        "process_content()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('PRESIDENT', 'NNP'), ('GEORGE', 'NNP'), ('W.', 'NNP'), ('BUSH', 'NNP'), (\"'S\", 'POS'), ('ADDRESS', 'NNP'), ('BEFORE', 'IN'), ('A', 'NNP'), ('JOINT', 'NNP'), ('SESSION', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('CONGRESS', 'NNP'), ('ON', 'NNP'), ('THE', 'NNP'), ('STATE', 'NNP'), ('OF', 'IN'), ('THE', 'NNP'), ('UNION', 'NNP'), ('January', 'NNP'), ('31', 'CD'), (',', ','), ('2006', 'CD'), ('THE', 'NNP'), ('PRESIDENT', 'NNP'), (':', ':'), ('Thank', 'NNP'), ('you', 'PRP'), ('all', 'DT'), ('.', '.')]\n",
            "[('Mr.', 'NNP'), ('Speaker', 'NNP'), (',', ','), ('Vice', 'NNP'), ('President', 'NNP'), ('Cheney', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('Congress', 'NNP'), (',', ','), ('members', 'NNS'), ('of', 'IN'), ('the', 'DT'), ('Supreme', 'NNP'), ('Court', 'NNP'), ('and', 'CC'), ('diplomatic', 'JJ'), ('corps', 'NN'), (',', ','), ('distinguished', 'JJ'), ('guests', 'NNS'), (',', ','), ('and', 'CC'), ('fellow', 'JJ'), ('citizens', 'NNS'), (':', ':'), ('Today', 'VB'), ('our', 'PRP$'), ('nation', 'NN'), ('lost', 'VBD'), ('a', 'DT'), ('beloved', 'VBN'), (',', ','), ('graceful', 'JJ'), (',', ','), ('courageous', 'JJ'), ('woman', 'NN'), ('who', 'WP'), ('called', 'VBD'), ('America', 'NNP'), ('to', 'TO'), ('its', 'PRP$'), ('founding', 'NN'), ('ideals', 'NNS'), ('and', 'CC'), ('carried', 'VBD'), ('on', 'IN'), ('a', 'DT'), ('noble', 'JJ'), ('dream', 'NN'), ('.', '.')]\n",
            "[('Tonight', 'NN'), ('we', 'PRP'), ('are', 'VBP'), ('comforted', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('hope', 'NN'), ('of', 'IN'), ('a', 'DT'), ('glad', 'JJ'), ('reunion', 'NN'), ('with', 'IN'), ('the', 'DT'), ('husband', 'NN'), ('who', 'WP'), ('was', 'VBD'), ('taken', 'VBN'), ('so', 'RB'), ('long', 'RB'), ('ago', 'RB'), (',', ','), ('and', 'CC'), ('we', 'PRP'), ('are', 'VBP'), ('grateful', 'JJ'), ('for', 'IN'), ('the', 'DT'), ('good', 'JJ'), ('life', 'NN'), ('of', 'IN'), ('Coretta', 'NNP'), ('Scott', 'NNP'), ('King', 'NNP'), ('.', '.')]\n",
            "[('(', '('), ('Applause', 'NNP'), ('.', '.'), (')', ')')]\n",
            "[('President', 'NNP'), ('George', 'NNP'), ('W.', 'NNP'), ('Bush', 'NNP'), ('reacts', 'VBZ'), ('to', 'TO'), ('applause', 'VB'), ('during', 'IN'), ('his', 'PRP$'), ('State', 'NNP'), ('of', 'IN'), ('the', 'DT'), ('Union', 'NNP'), ('Address', 'NNP'), ('at', 'IN'), ('the', 'DT'), ('Capitol', 'NNP'), (',', ','), ('Tuesday', 'NNP'), (',', ','), ('Jan', 'NNP'), ('.', '.')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}